\chapter{Maquina de Vetores Suporte}\label{chp:LABEL_CHP_2}

%\section{Definição}\label{sec:LABEL_CHP_2_SEC_A}
%silvana: icluir traducao parao portugues...
%silvana: sempre que usar termo em ingles, escrevê-lo em italico ({\em ...})
Maquina de Vetores de Suporte é uma classe de aprendizado de maquina que surgiu nos anos 90. Hoje em dia são usadas em diversas áreas como reconhecimento de escrita, bioinformática e mineração de dados.	Essencialmente eles são métodos de aprendizado supervisionado que podem ser usados para classificação binária, onde a partir de um conjunto de dados de treinamento a maquina aprende a classificar um novo exemplo como sendo de uma de duas classes, ou multi-classe, podendo classificar entre diversas classes, para isso o problema é subdividido em problemas binários menores. Também é possível usar uma MVS para regressão quando você quer estimar uma valor ou probabilidade invés de uma classe. 
%silvana: caracterizar classificacao e regressao, binario e multi-classe
%Felipe: melhor?
Por simplicidade vou abordar aqui os os classificadores binários, já que é preciso entender bem como ele funciona antes de entender os outros métodos. Sigo o fluxo de explicação abordado no livro \cite{art:LIVRO_SVM} que explica a teoria por tras da Maquina de Vetores de Suporte de forma bem didática.

\section{Superfícies de Decisão Lineares}
%silvana: sempre que usar termo em ingles, escrevê-lo em italico ({\em ...})
Uma forma simples de se criar um classificador é através de uma superficie de decião linear. Dado um conjunto de dados $(\bar{x}_i,y_i)$ onde $\bar{x}_i \in \mathbb{R}^n$ são os parâmetros de cada exemplo e $y_i \in \{ -1,+1 \}$ é a classe que você deseja classificar definida como $+1$ ou $-1$. Se o conjunto for linearmente separável é possível criar uma superfície de decisão linear (reta, plano ou hiperplano) que separa esse conjunto de dados.

Uma vez encontrado o plano separador a classificação fica fácil. Se definirmos esse plano como $\bar{d}\cdot \bar{x} = \bar{d}\cdot \bar{c}$ onde $\bar{d}$ é o vetor ortogonal ao plano, $\bar{c}$ é o vetor de deslocamento de $\bar{d}$ e $\bar{x}$ são os pontos pertencentes a reta. Podemos classificar um ponto $\bar{\alpha}$ como pertencendo a uma classe usando uma função discriminante \ref{eq:LABEL_EQ_1} onde a classe é definida pelo sinal do resultado.

\begin{equation}
f(\bar{\alpha})=sgn((\bar{\alpha}-\bar{c})\cdot\bar{d})
    \label{eq:LABEL_EQ_1}
\end{equation}

\begin{figure}
  \centering
  \includegraphics[width=0.4\textwidth]{imagens/svm_1.png}
  \caption{Classificação de um ponto $\bar{\alpha}$ por uma superfície de decisão linear}
  \label{fig:LABEL_FIG_1}
\end{figure}

%silvana: definir perceptron...
Como estamos assumindo que nossos dados são linearmente separáveis já sabemos qual a função discriminante, falta encontrar os parâmetros $\bar{c}$ e $\bar{d}$ que classificam corretamente esse conjunto de dados. Existem algumas formas de encontrar os parâmetros que definem essa superfície de decisão linear, um método próximo da maquina de vetores de suporte é o perceptron. O perceptron da valores aleatórios a $\bar{c}$ e $\bar{d}$ e classifica o conjunto de dados usando a função discriminante. A cada iteração o perceptron corrige os valores de $\bar{c}$ e $\bar{d}$ para os dados que estiverem classificados de forma errada. Dessa forma se o conjunto de dados for realmente linearmente separável o perceptron vai convergir para a superfície de decisão linear.

Essa superfície separa perfeitamente os exemplos usados no treinamento, mas pode ter muitos erros para classificar dados reais. Isso ocorre porque o perceptron dessa forma não tem nenhum critério para saber se a superfície é boa ou ruim. Usando a figura \ref{fig:LABEL_FIG_2} como exemplo vamos supor que os pontos +* e -* não estavam no conjunto de treinamento e o perceptron chegou na linha pontilhada como superfície de separação. A linha preta seria mais adequada já que divide o espaço de forma mais justa reduzindo a chance de ter um ponto classificado errado.

\begin{figure}
  \centering
  \includegraphics[width=0.4\textwidth]{imagens/svm_2.png}
  \caption{A linha pre representa a melhor separação e as linhas claras representam linhas com maior chance de erro}
  \label{fig:LABEL_FIG_2}
\end{figure}

\section{Classificadores de Margem Máxima}
Isso nos leva ao próximo conceito, classificadores de margem máxima. Essas técnicas tem como objetivo encontrar a superfície central entre dois conjuntos de dados (A linha preta, na figura anterior) para minimizar a chance de erro na classificação.

Como estamos procurando a margem máxima dado um conjunto de margens possíveis faz sentido abordar o problema como um problema de otimização.

Formalmente problemas de otimização são definidos como: $\underset{\bar{x}}{min}\phi(\bar{x})$ dado que $h_i(\bar{x})\ge c_i$ $\forall \bar{x} \in \mathbb{R}^n$. Onde $\phi: \mathbb{R}^n\rightarrow \mathbb{R}$ é a função objetivo que se quer minimizar e $h_i:\mathbb{R}^n\rightarrow\mathbb{R}$ é o limitante da função com limite $c_i$

\begin{figure}
  \centering
  \includegraphics[width=0.7\textwidth]{imagens/svm_3.png}
  \caption{Superfície de decisão ótima e seus vetores e hiperplanos de suporte}
  \label{fig:LABEL_FIG_3}
\end{figure}

Podemos então traduzir o problema de separar o conjunto de dados em um problema de otimização onde o objetivo é maximizar a distancia entre as superfícies de suporte. As superfícies de suporte são definidas como hiperplanos paralelos que tangenciam os conjuntos de dados de cada classe. E vetores de suporte são pontos pertencentes a essas superfícies. Assim o que nós queremos é a função da superfície de decisão ótima $\bar{w}^*\cdot\bar{x}=b^*$ onde a projeção entre os vetores de suporte $m^*=|\bar{x}_p - \bar{x}_q|cos\gamma$ é máximo.

\begin{figure}
  \centering
  \includegraphics[width=0.4\textwidth]{imagens/svm_4.png}
  \caption{Calculando a margem $m^*$ entre os planos de suporte}
  \label{fig:LABEL_FIG_4}
\end{figure}

Para prosseguir nossos cálculos vamos precisar rescrever $m^*$ como: $\frac{2k}{|\bar{w}^*|}$.

\begin{equation}
m^* = |\bar{x}_p-\bar{x}_q|cos\gamma
    \label{eq:LABEL_EQ_2}
\end{equation}

\begin{equation}
= \frac{\bar{w}^*\cdot(\bar{x}_p-\bar{x}_q)}{|\bar{w}^*|}
    \label{eq:LABEL_EQ_3}
\end{equation}

\begin{equation}
= \frac{\bar{w}^*\cdot\bar{x}_p-\bar{w}^*\cdot\bar{x}_q}{|\bar{w}^*|}
    \label{eq:LABEL_EQ_4}
\end{equation}

\begin{equation}
= \frac{(b^*+k)-(b^*-k)}{|\bar{w}^*|}
    \label{eq:LABEL_EQ_5}
\end{equation}

\begin{equation}
= \frac{2k}{|\bar{w}^*|}
    \label{eq:LABEL_EQ_6}
\end{equation}

\begin{equation}
\begin{split}
m^* &= |\bar{x}_p-\bar{x}_q|cos\gamma\\
    &= \frac{\bar{w}^*\cdot(\bar{x}_p-\bar{x}_q)}{|\bar{w}^*|}\\
    &= \frac{\bar{w}^*\cdot\bar{x}_p-\bar{w}^*\cdot\bar{x}_q}{|\bar{w}^*|}\\
    &= \frac{(b^*+k)-(b^*-k)}{|\bar{w}^*|}\\
    &= \frac{2k}{|\bar{w}^*|}
\end{split}
\end{equation}

Queremos $m^*$ máximo mas para facilitar nossos cálculos mais a frente vamos reescrever nossa função objetivo como:

\begin{equation}
\begin{split}
m^* &= max \frac{2k}{|\bar{w}|} \\
    &= min \frac{|\bar{w}|}{2k} \\
    &= min \frac{|\bar{w}|^2}{2k}  \\
    &= min \frac{1}{2k} \bar{w}\cdot\bar{w}  \\
    &= min \frac{1}{2} \bar{w}\cdot\bar{w}
\end{split}
\end{equation}

Nossas restrições podem ser escritas como:
%Felipe: essa notação ta certa o tal que? precisa de uma chave aí?
\begin{equation}
\begin{split}
\bar{w}^*\cdot\bar{x}_i \ge b^*+k \quad \forall (\bar{x}_i,y_i)\in D \quad | \quad y_i=+1 \\
\bar{w}^*\cdot\bar{x}_i \le b^*-k \quad \forall (\bar{x}_i,y_i)\in D \quad | \quad y_i=-1 \\
\bar{w}^*\cdot\bar{x}_i \ge 1+b \quad \forall (\bar{x}_i,y_i)\in D \quad | \quad y_i=+1 \\
\bar{w}^*\cdot(-\bar{x}_i) \ge 1-b \quad \forall (\bar{x}_i,y_i)\in D \quad | \quad y_i=-1 \\
\bar{w}^*\cdot(y_i\bar{x}_i) \ge 1+y_i b \quad \forall (\bar{x}_i,y_i)\in D
\end{split}
\end{equation}

Assim um classificador por margem máxima fica definido como: Dado um conjunto de dados linearmente separáveis $D=(\bar{x}_i,y_i) \subseteq \mathbb{R}^n\times\{+1,1\}$, podemos encontrar a superfície máxima de separação, $\bar{w}^*\cdot\bar{x}=b^*$, otimizando o problema $min\phi(\bar{w},b)=\underset{\bar{w},b}{min}\frac{1}{2}\bar{w}\cdot\bar{w}$ sujeito as restrições de $\bar{w}^*\cdot(y_i\bar{x}_i) \ge 1+y_i b \quad \forall (\bar{x}_i,y_i)\in D$.\par

Uma maquina de suporte de vetor é o problema dual do classificador de margem máxima. Podemos entender um problema dual como uma versão análoga do problema onde invés de encontrar o vetor de define a superfície de separação $w$, queremos encontrar o conjunto de valores de $a$ do qual podemos inferir $w$. Para resolver esse problema dual utilizamos a técnica dual Lagrangiana.

O método de otimização Lagrangiana reescreve um problema de otimização da forma:
\begin{equation}
\underset{\bar{x}}{min}\phi(\bar{x}) \quad \text{dado que} \quad h_i(\bar{x})\ge c_i
\end{equation}
como:
\begin{equation}
\underset{\bar{\alpha}}{max} \underset{\bar{x}}{min} L(\bar{\alpha},\bar{x}) = \underset{\bar{\alpha}}{max} \underset{\bar{x}}{min}\bigg(\phi(\bar{x})-\sum_{i=1}^{l}\alpha_i g_i(\bar{x})\bigg)
    \label{eq:LABEL_EQ_7}
\end{equation}
dado que
\begin{equation}
\alpha_i\ge0
    \label{eq:LABEL_EQ_8}
\end{equation}
Encontrados os valores máximo de $a^*$ e minimo de $x^*$ a solução do problema dual vai ser o mesmo do problema primal contanto que as seguintes condições se apliquem:

\begin{equation}
\begin{split}
\frac{\partial L}{\partial \bar{x}}(\bar{\alpha}^*,\bar{x})&=\bar{0}, \\
\alpha_i^*g_i(\bar{x}^*)&=\bar{0}, \\
g_i(\bar{x}^*)&\ge\bar{0} , \\
\bar{\alpha}_i^*(\bar{x}^*)&\ge\bar{0} 
\end{split}
\end{equation}

Essas condições são conhecidas como as condições de Karush-Kuhn-Tucker ou KKT e podemos usar elas para reescrever nosso problema dependendo somente de $\alpha$ facilitando nossa busca. Agora podemos aplicar essa técnica no nosso problema de margem máxima, escrevemos nossa função lagrangiana como:

\begin{equation}
\begin{split}
L(\bar{\alpha},\bar{w},b) &=\phi(\bar{w},b) -  \sum_{i=1}^{l}\alpha_i g_i (\bar{w},b) \\
 &=\frac{1}{2}\bar{w}\cdot\bar{w} - \sum_{i=1}^{l}\alpha_i (y_i(\bar{w}\cdot \bar{x}_i -b)-1) \\
 &=\frac{1}{2}\bar{w}\cdot\bar{w} - \sum_{i=1}^{l}\alpha_i y_i \bar{w}\cdot \bar{x}_i + b \sum_{i=1}^{l}\alpha_i y_i + \sum_{i=1}^{l}\alpha_i\\
\end{split}
\end{equation}

Aplicando as KKTs chegamos no nosso classificador de margem máxima dual:
\begin{equation}
    \underset{\bar{\alpha}}{max} \phi' (\bar{\alpha}) = \underset{\bar{\alpha}}{max} \Bigg( \sum_{i=1}^{l}\alpha_i - \frac{1}{2}\sum_{i=1}^{l}\sum_{j=1}^{l}\alpha_i \alpha_j y_i y_j \bar{x}_i\cdot \bar{x}_j \Bigg)
    \label{eq:EQ_Treinador_1}
\end{equation}
dado que:
\begin{equation}
    \sum_{i=1}^{l}\alpha_i y_i = 0 \quad \text{e} \quad \alpha_i \ge 0 \forall \quad i \in \{1,l\}
\end{equation}
Uma vez encontrado nosso $\bar{\alpha}^*$ ótimo podemos classificar um novo ponto $\bar{x}$ com a formula:
\begin{equation}
    f(\bar{x}) = sgn\Bigg(
        \sum_{i=1}^{l} \alpha_i^*y_i\bar{x}_i\cdot\bar{x}
        -b^*
    \Bigg)
    \label{eq:EQ_Classificador_1}
\end{equation}

\begin{equation}
    b^* = \sum_{i=1}^{l} \alpha_i^*y_i\bar{x}_i\cdot\bar{x}_{sv+}
        -1
    \Bigg)
    \label{eq:EQ_B_1}
\end{equation}

Onde $b^*$ representa a distancia da origem do plano, $\alpha_i^*=0$ implica que $\bar{x}_i$ não influencia no resultado, $\alpha_j^*> 0$ implica que $\bar{x}_j$ é um vetor de suporte e $\bar{x}_{sv+}$ é um vetor de suporte positivo qualquer.


\section{O Truque do Kernel}
Com isso conseguimos uma maquina de vetores de suporte linear. Acontece que poucos conjuntos de dados na pratica são linearmente separáveis. Para aplicar nossa maquina em um conjunto de dados que não seja linearmente separáveis precisamos projeta-los de forma que fiquem linearmente separáveis, como na figura \ref{fig:LABEL_FIG_5}

\begin{figure}
  \centering
  \includegraphics[width=1\textwidth]{imagens/svm_5.png}
  \caption{Em (a) não é possível separar os conjuntos de forma linear, mas transformando os dados em $\mathbb{R}^2\rightarrow \mathbb{R}^3$ em (b) conseguimos separa-los linearmente}
  \label{fig:LABEL_FIG_5}
\end{figure}

Para Isso precisamos escolher funções de projeção $ \phi:\mathbb{R}^n\rightarrow\mathbb{R}^m$. Assim passamos todos nossos exemplos para um espaço de dimensão maior na esperança que nesse espaço eles possam ser linearmente separáveis. 

Podemos ver nas equações de treinamento \ref{eq:EQ_Treinador_1} e classificação \ref{eq:EQ_Classificador_1} que sempre é avaliado o produto de dois conjuntos de dados $\bar{x}_i\cdot\bar{x}_j$, então podemos substituir $\phi(\bar{x}_i) \cdot \phi ( \bar{x}_j )$ por $k(\bar{x}_i,\bar{x}_j)$ de forma que que seja mais simples que o produto de $\phi(\bar{x})$.

Na figura \ref{fig:LABEL_FIG_5} $\phi(\bar{x})=(x_1^2,x_2^2,\sqrt{2}x_1x_2)$ e podemos simplificar seu produto:

\begin{equation}
\begin{split}
\phi(\bar{x})\cdot \phi(\bar{y}) &= (x_1^2,x_2^2,\sqrt{2}x_1x_2) \cdot (y_1^2,y_2^2,\sqrt{2}y_1y_2) \\
&=x_1^2y_1^2+x_2^2y_2^2+2x_1x_2y_1y_2 \\
&=(x1y1+x2y2)(x1y1+x2y2) \\
&=(\bar{x}\cdot\bar{y})(\bar{x}\cdot\bar{y}) \\
&=(\bar{x}\cdot\bar{y})^2
\end{split}
\end{equation}

Chamamos essa simplificação de Truque de Kernel. Existe um estudo grande sobre Kernels validos e uma seria de restrições que uma função tem que cumprir para ser considerada um Kernel válido. Não me aprofundarei nesse assunto aqui. Muitos desses kernels possuem uma variável livre que deve ser adaptada ao conjunto de dados estudado podendo influenciar na sua precisão. Alguns kernels populares e suas variáveis livres são:
\begin{table}
    \centering
    \begin{tabular}{c|c|c}
        Kernel & Função & Parametros Livres \\ \hline
        Kernel Linear & $k(\bar{x},\bar{y})=\bar{x}\cdot\bar{y}$ & nenhum \\
        Kernel Homogeneo Polinomial & $k(\bar{x},\bar{y})=(\bar{x}\cdot\bar{y})^2$ & $d\ge2$ \\
        Kernel Não-Homogeneo Polinomial & $k(\bar{x},\bar{y})=(\bar{x}\cdot\bar{y}+c)^2$ & $d\ge2, c > 0$ \\
        Kernel Gaussiano & $k(\bar{x},\bar{y})=e^{-\big(\frac{|\bar{x}-\bar{y}|^2}{2\sigma^2}\big)}$ & $\sigma>0$ \\ \hline
    \end{tabular}
    \caption{Kernels Populares e suas váriaveis livres}
    \label{tab:Kernels}
\end{table}

Para transformar nossa maquina de vetor de suporte linear em um não linear apenas substituímos $\phi(\bar{x}_i) \cdot \phi ( \bar{x}_j )$ por $k(\bar{x}_i,\bar{x}_j)$ em \ref{eq:EQ_Treinador_1}, \ref{eq:EQ_Classificador_1} e \ref{eq:EQ_B_1}.

Treinador:
\begin{equation}
    \bar{\alpha}^* = \underset{\bar{\alpha}}{argmax} \Bigg( \sum_{i=1}^{l}\alpha_i - \frac{1}{2}\sum_{i=1}^{l}\sum_{j=1}^{l}\alpha_i \alpha_j y_i y_j k(\bar{x}_i,\bar{x}_j) \Bigg)
    \label{eq:EQ_Treinador_2}
\end{equation}

Classificador:
\begin{equation}
    f(\bar{x}) = sgn\Bigg(
        \sum_{i=1}^{l} \alpha_i^*y_i k(\bar{x}_i,\bar{x})
        -b^*
    \Bigg)
    \label{eq:EQ_Classificador_2}
\end{equation}
\begin{equation}
    b^* = \sum_{i=1}^{l} \alpha_i^*y_i k(\bar{x}_i,\bar{x}_{sv+})
        -1
    \Bigg)
    \label{eq:EQ_B_2}
\end{equation}

\section{Margem Flexível}
Conjuntos reais de dados possuem ruídos, e pontos anormais podem distorcer e piorar muito a performance de uma uma maquina de vetor de suporte. As maquinas vistas até agora são considerados com margem rígida, mas também existem máquinas de vetor de suporte que ignoram certos pontos dentro de um ruído considerados com margem flexível.

\begin{figure}
  \centering
  \includegraphics[width=1\textwidth]{imagens/svm_6.png}
  \caption{No exemplo da direita ignoramos $\bar{x}_j$ que pode ser considerado ruído}
  \label{fig:LABEL_FIG_6}
\end{figure}

Para isso escolhemos uma constante $C$ que atribuímos como um peso para os erros. No problema primal definimos $C$ como $\underset{\bar{w},\bar{\epsilon},b}{min}\phi(\bar{w},\bar{\epsilon},b) = \frac{1}{2}\bar{w}^*\cdot\bar{w}^* + C\sum_{i=1}^l \epsilon_i^*=m^*$ de forma que quanto maior $C$ maior o peso dos erros, mais próximo a margem flexível fica da margem rígida. Um $C$ menor permite mais erros fazendo com que a margem cresça e se distancie da margem rígida. No problema dual esse limite aparece como: $0\le \alpha_i \le C$

\section{Implementação}
Com isso definimos uma maquina de vetor de suporte bem robusto capaz de classificar conjuntos de dados não lineares e suscetíveis a ruídos. A complexidade de implementação de uma MVS consiste na dificuldade de se achar o valor máximo de $\bar{\alpha}$ existem alguns algoritmos diferentes para se chegar nesse resultado, alguns deles descritos em \cite{art:LIVRO_SVM} são:
%Felipe No livro está Gradient Ascent, não sei se essa tradução está boa
\subsection{Incremento de Gradiente}
O gradiente de um vetor é um vetor que aponta para a direção onde a função cresce naquele ponto. Dessa forma podemos calcular o gradiente de $\alpha$ em um ponto aleatório e e incrementar os valores de $\alpha$ incrementalmente. Quando o gradiente for $0$ chegamos no máximo local e encontramos o valor ótimo de $\alpha$. Apesar de não considerarmos $b$ nessa implementação seu valor pode ser encontrado a partir do valor de $\alpha$.
\subsection{Algoritmo Kernel-Adatron}
O problema do incremento de gradiente é que ele ignora as restrições de otimização definidas por KKT e não converge e não considera o valor de $b$ que pode mudar o ponto máximo. O algoritmo Kernel-Adatron resolve esses dois problemas. Podemos fixar o valor de $b$ como zero, dessa forma ao garantirmos que o valor $\alpha$ é maior que zero garantimos também que as condições de KKT são válidas.
\begin{algorithm}
\label{alg:KAA}
\caption{Algoritmo Kernel-Adatron}
\begin{algorithmic}
\STATE $D = \{(\bar{x_i},y_1)...(\bar{x_i},y_1)\} \subset \mathbb{R}^{n}\times \{+1,-1\}$
\STATE $\eta>0$
\STATE $C>0$
\STATE $b=0$
\STATE $\bar{\alpha} \leftarrow \bar{0}$
\REPEAT
\STATE $\bar{\alpha_{old}} \leftarrow \bar{\alpha}$
\FOR{$i=1$ to $l$}
\STATE $\alpha_i \leftarrow min\big\{C,max\big\{0,\alpha_i + \eta-\eta y_i \sum_{j=1}^l y_j \alpha_j k(\bar{x}_j,\bar{x}_i))\big\}\big\}$
\ENDFOR
\UNTIL{$\bar{\alpha}-\bar{\alpha_{old}}\approx \bar{0}$}
\RETURN $(\bar{\alpha},b)$
\end{algorithmic}
\end{algorithm}

\subsection{Quadratic Program Solver}
Como a forma de se encontrar os valores de $\bar{\alpha}$ é um problema de otimização existem métodos genericos de otimização que ajudam nessa tarefa um deles é o \emph{Quadratic Program Solver}, um algoritmo que encontra os valores de um parâmetro que minimizam uma função dado um conjunto de restrições. Varias bibliotecas implementam esse algoritmo de forma que seria possivel resolver a SVM sem ter que implementar o algoritmo de otimização.
\subsection{SMO: Sequential Minimal Optimization}
Em vez de tentar encontrar todos os valores de $\alpha$ de uma vez o \emph{Sequential Minimal Optimization} escolhe dois pontos de treinamento, otimiza eles e vê se as restrições de KKT são válidas para o resto do conjunto de treinamento, caso contrário escolhe outros dois pontos e continua o algoritmo. Os pontos escolhidos são sempre de forma que um ponto respeite as restrições e o maior violador das restrições. Dessa forma é possível provar que o algoritmo vai convergir. Isso permite uma serie de otimizações mais complexas em cima das formulas vistas de forma que o calculo final fique mais simples e econômico.

\subsection{Escolhido: KAA}
O algoritmo que acabei escolhendo foi o KAA, sua implementação é só um pouco mais complexa que o de incremento de gradiente e ainda tem um fluxo fácil de compreender. Não escolhi o \emph{Quadratic Program Solver} pois mudaria o foco do projeto para um algoritmo mais genérico de otimização. Apesar de o SMO ser o algoritmo mais usado nos artigos em que me baseei achei que seria interessante implementar primeiro o KAA já que seria mais fácil de entender como o algoritmo está convergindo o que as variáveis representam no resultado final. Além disso por o KAA ser um algoritmo mais custoso espero um ganho mais significativo da versão sequencial para a paralela de forma que seria um objeto de estudo mais interessante.