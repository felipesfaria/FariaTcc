\chapter{Programação Paralela com GPU}\label{chp:LABEL_CHP_3}

Até o início dos anos 2000 o aumento na capacidade de processamento dos computadores costumava acontecer pelo aumento de frequência de processamento das CPUs. Esse aumento foi interrompido pois não era mais possível aumentar o processamento mantendo a dissipação de calor. A forma que a industria encontrou de manter o crescimento de processamento dos computadores sem poder aumentar a frequência foi através da programação paralela, os CPUs passaram a ter mais de um núcleo capaz de rodar diferentes instruções simultaneamente, assim a quantidade total de processamento continuou a crescer.

O conceito de programação paralela consiste em distribuir o processamento de um programa com a finalidade de reduzir o tempo total de processamento. Isso pode ser feito de algumas formas. Os programas podem executar em diferentes núcleos de um mesmo computador dividindo o processamento em diferentes threads ou processos, pode ser dividido em diferentes máquinas através de um cluster ou computação em nuvem e pode ser dividido entre uma CPU e aceleradores, como por exemplo uma GPU que é o foco desse trabalho.

O uso de arquiteturas diferentes em um programa não é trivial e geralmente exige um conhecimento aprofundado das capacidades de cada unidade de processamento. Por esse motivo isso não é feito de forma automática, o uso das arquiteturas é feito em chamadas explicitas. Não basta encaixar uma GPU ou FPGA na sua placa mãe e esperar que seu programa tire proveito desse hardware, nem é possível simplesmente dizer para um programa feito para executar em CPU executar em uma GPU. Mesmo que fosse possível, não é verdade que todo algoritmo rode melhor em um acelerador já que eles são otimizados para casos de uso específico.

\section{GPUs}
GPUs, também conhecidas como placas de vídeo, originalmente eram conhecidas como aceleradores gráficos. Seu objetivo era o de processar imagens rapidamente. Algumas das aplicações necessárias são alterar a cor de todos os pixels de uma imagem, rotação e translação de polígonos altamente complexos com centenas de pontos, aplicação de texturas e calculo de luminosidade nesses Polígonos. Fica claro ver que em muitas das aplicações necessárias para uma placa de vídeo acelerar o processamento de imagens é necessário que a mesma operação seja feita em diversos pontos. Como o seu foco é claramente diferente das CPUs, quem tem uma enfase muito maior em controle de fluxo e cache de dados, sua arquitetura também é diferente. Para se aproveitar desse paralelismo as GPUs possuem centenas ou até milhares de unidades logicas, que podem rodar as mesmas operações sobre blocos distintos de memória. Como podemos ver na figura \ref{fig:gpu_1}, que modela o nucelo de uma CPU e uma GPU, a CPU tem muito mais espaço para controle e cache e a GPU dedica a maior parte do seu núcleo para as unidades logicas aritméticas.

\begin{figure}
  \centering
  \includegraphics[width=1\textwidth]{imagens/gpu_1.png}
  \captionsource{As GPUs dedicam mais transistores ao processamento de dados}{\cite{CUDA}}
  \label{fig:gpu_1}
\end{figure}

Inicialmente o foco principal dessas placas era na industria de jogos e esse mercado bilionário impulsionou uma rápida evolução na capacidade das GPUs. Não demorou muito tempo para que os desenvolvedores em geral vissem que essas aplicações não são limitadas a industria de jogos. Inicialmente era muito trabalhoso aplicar a capacidade das placas de vídeo para aplicações gerais pois era necessário conhecer APIs especificas de processamento de imagem e saber traduzir os problemas de um domínio para o outro. Para que os desenvolvedores pudessem se aproveitar desse processamento para outros fins, os produtores de GPUs tem disponibilizados bibliotecas para executar operações de uso geral na GPU. Como o CUDA distribuído pela NVIDIA para ser usado em suas placas gráficas.

\section{CUDA}
Lançado em 2006 CUDA é plataforma de computação e modelo de programação proprietária da NVIDIA feita para desenvolvedores poderem explorar o potencial das suas GPUs para aplicações gerais, atualmente está na versão 7.5. CUDA foi desenvolvido para ser simples e facilmente compreensível por pessoas com familiaridade com linguagens de programação como C. Com o CUDA é possível enviar código em C, C++ e fortran diretamente para GPU sem precisar escrever em assembly. Nesse trabalho, não estarei abordando o uso do fortran.
%silvana: há versões de Cuda também para Fortran, então melhor se referir a CUDA C para falar especificamente da API de Cuda pra C
A base do CUDA consiste de 3 abstrações hierarquia de threads, memória compartilhada e sincronização que estão acessíveis ao programador através de comandos básicos. 
A escalabilidade do CUDA é feita de forma transparente para o desenvolvedor, basta fazer o programa de forma realmente paralela onde um bloco não precise de informação de outro bloco e a ordem de execução não importa. A execução dos blocos é distribuída em tempo de execução de forma a explorar melhor a arquitetura.

\subsection{Modelo de programação}
%silvana: o kernel não necessariamente é escrito em C (apenas no caso de estar usando CUDA C)
O núcleo do CUDA consiste de kernels, funções escritas em C que são compiladas para ser executadas na GPU. O kernel é definido pela declaração $\_\_global\_\_$ e o número de threads e blocos que vai executar a função é especificada quando a função é chamada pela sintaxe $nomeDaFuncao<<<blocos, threads>>>(parametros...)$. Blocos são conjuntos de threads que são executados no mesmo core e compartilham memória. Atualmente nenhuma placa de vídeo suporta mais de 1024 threads por bloco, esses valores precisam ser levadas em consideração pelo desenvolvedor. Para executar a soma de dois vetores de 2048 elementos por exemplo seria possível separar em 4 blocos de 512 threads.
%silvana ...ou em dois de 1024, ou em 8 de 256...dar mais exemplos...
Para identificar qual thread vai processar qual dado, cada thread possui um id de thread, $threadIdx$, e de bloco, $blockIdx$, e é possível saber o índice do vetor que se quer acessar fazendo $indice = threadIdx.x + blockIdx.x * blockDim.x$. 
%silvana: descrever  acima o que é threadidx.x etc
Cada kernel tem acesso a memória privada, compartilhada por bloco e global(compartilhada por todas as threads).

%silvana: fazer referência ao código no texto, explicando como ele funciona
%silvana: corrigir o código, está definindo o kernel Square e chamando VecAdd
\codec{Como escrever e chamar um Kernel}{alg:cuda_1}{codigos/cuda_1.txt}

%silvana: melhor dizer (depois de heterogênea...) que parte do programa executa na CPU (host) e outra parte na GPU (device)
CUDA é um modelo de programação heterogênea, isso implica que a CPU (chamada de $host$) e a GPU(chamada de $device$) executam códigos distintos e possuem memória distinta. Por isso é necessário no programa sequencial especificar para GPU como administrar a memória e quando executar os kernels. 
%silvana: não necessariamente será uma programa sequencial na CPU, então melhor dizer que Cuda oferece funções que são executadas do lado da CPU para alocar memória na GPU, transferir dados da CPU par a GPU e disparar a execução dos kernels na GPU
A memoria pode ser alocada na GPU através de $cudaMalloc(\&dvcPtr, tamanho)$. É preciso salvar a referencia do ponteiro da GPU pois é esse ponteiro que vai ser passado para as chamadas de kernel. Para copiar informação do $host$ para o $device$ é preciso usar a função $cudaMemcpy(dvcPtr, hstPtr, tamanho,  tipo)$, tipo é um enum e pode ser $cudaMemcpyHostToDevice$ ou $cudaMemcpyHostToDevice$ entre outros. Para liberar a memória alocada no $device$ usa-se $cudaFree(devicePtr)$. 
%silvana: deixar claro que são assíncronas em relação a CPU (em relação a GPU se todas as invocações estiverem no mesmo stream na GPU elas serão executadas uma após a outra). Se for necessário aguardar o final do kernel antes de executar alguma outra coisa na CPU, aí então é necessario chamar uma função de sincronização para forcar a espera do kernel. Também destacar que algumas funções da GPU, como alocação de memória e transferência de dados já são bloqueantes do lado da CPU.
As chamadas de kernel por default são assíncronas, se você precisa garantir que um kernel terminou antes de fazer outra coisa, é necessário chamar $cudaDeviceSynchronize()$, essa função vai esperar que as operações que estão executando no $device$ terminem.

%silvana: nesse exemplo não precisa chamar cudaDeviceSynchronize depois de invocar o kernel pois a chamada de cudaMemcpy() já irá esperar o kernel terminar para ser executada...A chamada do kernel está incompleta...
%silvana: lembrar de citar o código no texto e descrever o que ele faz

\codec{Como alocar memória e copiar dados para GPU}{alg:cuda_2}{codigos/cuda_2.txt}

%silvana: "cuda" sempre maiúsculo
Como as funções de cuda não rodam na CPU as vezes é difícil saber o que está acontecendo e se ocorreu um erro ou se o programa está em um estado inválido do lado da GPU. Por isso as funções de cuda retornam códigos de erro ou sucesso definidos pelo enum $cudaError$. O kernel não possui esse retorno então é possível checar o status da GPU após as chamadas de kernel para saber se ocorreu um erro com a função $cudaGetLastError()$.

É possível incluir funções dentro de kernels de CUDA, para isso é preciso colocar a diretiva $\_\_device\_\_$ antes da função, assim o compilador vai saber que essa função deve ser executada em GPU e irá compilar de acordo. Caso queira que a função também possa rodar na CPU é possível colocar a diretiva $\_\_host\_\_$, assim usando as duas diretivas é possível usar a mesma função em códigos dentro e fora de kernels de CUDA.
